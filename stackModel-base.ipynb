{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本库import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-*- encoding:utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, accuracy_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入通话记录，短信记录，访问记录数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入用户通话记录\n",
    "names_voice = ['uid','opp_num','opp_head','opp_len','start_time','end_time','call_type','in_out']\n",
    "voice_data = pd.read_table(\"./data/train/voice_train.txt\",sep='\\t',header=None,encoding='utf-8',names = names_voice,index_col = False,low_memory=False)\n",
    "\n",
    "# 导入用户短信记录\n",
    "names_sms = ['uid','opp_num','opp_head','opp_len','start_time','in_out']\n",
    "sms_data = pd.read_table(\"./data/train/sms_train.txt\",sep='\\t',header=None,encoding='utf-8',names = names_sms,index_col = False,low_memory=False)\n",
    "\n",
    "# 导入用户通话记录\n",
    "names_wa = ['uid','wa_name','visit_cnt','visit_dura','up_flow','down_flow','wa_type','date']\n",
    "wa_data = pd.read_table(\"./data/train/wa_train.txt\",sep='\\t',header=None,encoding='utf-8',names = names_wa,index_col = False,low_memory=False)\n",
    "\n",
    "# 读取训练与测试数据\n",
    "uid_label = pd.read_table(\"./data/train/uid_train.txt\",sep='\\t',header=None,names=['uid','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 对用户的电话接拨情况统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVoiceFeature(data):\n",
    "    ## 每个用户的电话总数量 丢\n",
    "    voice_count = data.groupby('uid')['in_out'].count()\n",
    "    voice_count.fillna(0,inplace=True)\n",
    "    ## 每个用户收/发电话的总数\n",
    "    voice_count_by_inout = data.groupby(['uid','in_out'])['opp_len'].count().unstack('in_out').rename(columns={0:'send_voice_cnt',1:'recv_voice_cnt'})\n",
    "    voice_count_by_inout.fillna(0,inplace=True)\n",
    "    ## 每个用户收/发电话的号码的平均长度\n",
    "    voice_mean_opp_len_by_inout = data.groupby(['uid','in_out'])['opp_len'].mean().unstack('in_out').rename(columns={0:'send_voice_opplen',1:'recv_voice_opplen'})\n",
    "    voice_mean_opp_len_by_inout.fillna(0,inplace=True)\n",
    "    ## 每个用户通话的平均时长和最长时长 丢\n",
    "    data['dura']=abs(data.end_time-data.start_time)\n",
    "    voice_mean_dura = data.groupby('uid')['dura'].mean().rename(columns={1:'mean_dura'})\n",
    "    voice_max_dura = data.groupby('uid')['dura'].max().rename(columns={1:'max_dura'})\n",
    "\n",
    "    ## 每个用户每种通话类型的次数\n",
    "    data['call_type'] = data['call_type'].astype('category')\n",
    "    voice_count_by_type = data.groupby(['uid','call_type'])['opp_len'].count().unstack('call_type').rename(columns={1:'Cbendi',2:'Cshengnei',3:'Cshengji',4:'Cgangaotai',5:'Cguoji'})\n",
    "    voice_count_by_type.fillna(0,inplace=True)\n",
    "    ## 每个用户每种通话类型的平均时长\n",
    "    data['call_type'] = data['call_type'].astype('category')\n",
    "    voice_dura_by_type = data.groupby(['uid','call_type'])['dura'].mean().unstack('call_type').rename(columns={1:'Dbendi',2:'Dshengnei',3:'Dshengji',4:'Dgangaotai',5:'Dguoji'})\n",
    "    voice_dura_by_type.fillna(0,inplace=True)\n",
    "    ## 结合数据\n",
    "    voice = pd.concat([voice_count_by_inout,voice_mean_opp_len_by_inout,voice_max_dura,voice_count_by_type,voice_dura_by_type],axis =1).reset_index().rename(columns={'in_out':'sms_total_cnt',0:'mean_dura',1:'max_dura'})\n",
    "    return voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 对用户的短信收发情况统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSmsFeature(data):\n",
    "    ## 每个用户的短信总数量 丢\n",
    "    sms_count = data.groupby('uid')['in_out'].count()\n",
    "    sms_count.fillna(0,inplace=True)\n",
    "    ## 每个用户收/发短信的总数\n",
    "    sms_count_by_inout = data.groupby(['uid','in_out'])['opp_len'].count().unstack('in_out').rename(columns={0:'send_sms_cnt',1:'recv_sms_cnt'})\n",
    "    sms_count_by_inout.fillna(0,inplace=True)\n",
    "    ## 每个用户收/发短信的号码的平均长度\n",
    "    sms_mean_opp_len_by_inout = data.groupby(['uid','in_out'])['opp_len'].mean().unstack('in_out').rename(columns={0:'send_sms_opplen',1:'recv_sms_opplen'})\n",
    "    sms_mean_opp_len_by_inout.fillna(0,inplace=True)\n",
    "    ## 结合数据\n",
    "    sms = pd.concat([sms_count_by_inout,sms_mean_opp_len_by_inout],axis =1).reset_index().rename(columns={'in_out':'sms_total_cnt'})\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 对用户的W/A访问情况统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWaFeature(data):\n",
    "    data['wa_name'] = data['wa_name'].astype('category')\n",
    "    #每个用户的总访问时长 丢\n",
    "    wa_dura_sum = data.groupby('uid')['visit_dura'].sum()\n",
    "    wa_dura_sum.fillna(0,inplace=True)\n",
    "    ## 每个用户web/APP时长\n",
    "    wa_dura_by_type = data.groupby(['uid','wa_type'])['visit_dura'].sum().unstack('wa_type').rename(columns={0.0:'web_dura',1.0:'APP_dura'})\n",
    "    wa_dura_by_type.fillna(0,inplace=True)\n",
    "    ## 每个用户web/APP上行流量\n",
    "    wa_up_flow_by_type = data.groupby(['uid','wa_type'])['up_flow'].sum().unstack('wa_type').rename(columns={0.0:'web_up_flow',1.0:'APP_up_flow'})\n",
    "    wa_up_flow_by_type.fillna(0,inplace=True)\n",
    "    ## 每个用户web/APP下行流量\n",
    "    wa_down_flow_by_type = data.groupby(['uid','wa_type'])['down_flow'].sum().unstack('wa_type').rename(columns={0.0:'web_down_flow',1.0:'APP_down_flow'})\n",
    "    wa_down_flow_by_type.fillna(0,inplace=True)\n",
    "    ## 结合数据\n",
    "    wa = pd.concat([wa_dura_by_type,wa_up_flow_by_type,wa_down_flow_by_type],axis =1).reset_index().rename(columns={0:'visit_dura_total','index':'uid'})\n",
    "    return wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeature(voice,sms,wa,uid_label):\n",
    "    #voice = getVoiceFeature(voice_data)\n",
    "    #sms = getSmsFeature(sms_data)\n",
    "    #wa = getWaFeature(wa_data)\n",
    "    fetures = uid_label.merge(voice,how='outer',right_on='uid',left_on='uid')\n",
    "    fetures = fetures.merge(sms,how='outer',right_on='uid',left_on='uid')\n",
    "    fetures = fetures.merge(wa,how='outer',right_on='uid',left_on='uid')\n",
    "    fetures.fillna(0,inplace=True)\n",
    "    return fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voice = getVoiceFeature(voice_data)\n",
    "sms = getSmsFeature(sms_data)\n",
    "wa = getWaFeature(wa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 划分训练and测试\n",
    "uid_label_train, uid_label_test = train_test_split(uid_label)\n",
    "voice_train = voice.loc[voice.uid.isin(uid_label_train['uid'])]\n",
    "voice_test = voice.loc[voice.uid.isin(uid_label_test['uid'])]\n",
    "\n",
    "sms_train = sms.loc[sms.uid.isin(uid_label_train['uid'])]\n",
    "sms_test = sms.loc[sms.uid.isin(uid_label_test['uid'])]\n",
    "\n",
    "wa_train = wa.loc[wa.uid.isin(uid_label_train['uid'])]\n",
    "wa_test = wa.loc[wa.uid.isin(uid_label_test['uid'])]\n",
    "\n",
    "#uid_label_train = uid_label.loc[index_train]\n",
    "#uid_label_test = uid_label.loc[index_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "train = getFeature(voice_train,sms_train,wa_train,uid_label_train)\n",
    "test = getFeature(voice_test,sms_test,wa_test,uid_label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_train为训练集的特征，X_test为测试集的特征，y_train是训练集的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['uid','label'],axis=1)\n",
    "X_test = test.drop(['uid','label'],axis=1)\n",
    "y_train = train.label\n",
    "y_test = test.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb 参数配置，自定义评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'booster':'gbtree',\n",
    "    'objective':'multi:softmax',\n",
    "    'stratified':True,\n",
    "    'max_depth':10,\n",
    "    # 'gamma':1,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.8,\n",
    "    # 'lambda':1,\n",
    "    'eta':0.5,\n",
    "    'seed':20,\n",
    "    'silent':1,\n",
    "    'num_class':2\n",
    "}\n",
    "def evalScore(preds,dtrain):\n",
    "    label = dtrain.get_label()\n",
    "    return 'sco',0.4*f1_score(label,preds,average='weighted')+0.6*accuracy_score(label,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线下cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dtrain = xgb.DMatrix(X_train,label=y_train)\n",
    "#xgb.cv(xgb_params,dtrain,num_boost_round=200,nfold=3,verbose_eval=10,\n",
    "#       early_stopping_rounds=100,maximize=True,feval=evalScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model=xgb.train(xgb_params,dtrain=dtrain,num_boost_round=190,verbose_eval=10,\n",
    "#                evals=[(dtrain,'train')],maximize=True,feval=evalScore,early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n",
    "# Class to extend XGboost classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    #'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "# Create 5 objects that represent our 4 models\n",
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n",
    "\n",
    "#准备训练测试集\n",
    "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "y_train = train['label'].ravel()\n",
    "x_train = train.drop(['uid','label'], axis=1).values # Creates an array of the train data\n",
    "x_test = test.drop(['uid','label'], axis=1).values # Creats an array of the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 防止过拟合 划分训练测试集\n",
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhuiling/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n",
    "\n",
    "#print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhuiling/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04774083 0.09820526 0.01002945 0.07442401 0.03250899 0.06858337\n",
      " 0.00711092 0.01298623 0.         0.         0.04864563 0.01856477\n",
      " 0.00824039 0.         0.00048659 0.0725926  0.04803358 0.02078582\n",
      " 0.24093538 0.03925635 0.03137885 0.03477325 0.01891428 0.04059493\n",
      " 0.02520853]\n",
      "[3.24950308e-02 7.99269467e-02 4.05919367e-02 1.16694531e-01\n",
      " 2.47661437e-02 6.54640454e-02 2.06063132e-02 1.34169814e-02\n",
      " 1.99689888e-05 9.66595192e-04 2.51712469e-02 1.66939956e-02\n",
      " 6.95511935e-03 4.71062388e-06 1.10996939e-03 1.38400210e-01\n",
      " 3.02824640e-02 2.14685703e-02 1.74075666e-01 4.77113014e-02\n",
      " 4.12953679e-02 1.99072744e-02 2.07209630e-02 3.89373113e-02\n",
      " 2.23173370e-02]\n",
      "[0.05  0.036 0.026 0.062 0.08  0.026 0.03  0.03  0.    0.002 0.052 0.04\n",
      " 0.034 0.    0.006 0.028 0.072 0.012 0.106 0.064 0.032 0.046 0.064 0.064\n",
      " 0.038]\n",
      "[0.03902348 0.0428943  0.03142478 0.05566559 0.08747559 0.05058702\n",
      " 0.0209939  0.0221466  0.         0.0004623  0.07231797 0.01735743\n",
      " 0.03740785 0.         0.00048245 0.02858187 0.05079612 0.02417045\n",
      " 0.09325172 0.06387994 0.03751098 0.05670123 0.04332294 0.06390962\n",
      " 0.05963588]\n"
     ]
    }
   ],
   "source": [
    "# 得出特征重要性评估\n",
    "rf_feature = rf.feature_importances(x_train,y_train)\n",
    "et_feature = et.feature_importances(x_train, y_train)\n",
    "ada_feature = ada.feature_importances(x_train, y_train)\n",
    "gb_feature = gb.feature_importances(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二层模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>ExtraTrees</th>\n",
       "      <th>GradientBoost</th>\n",
       "      <th>RandomForest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3749.000000</td>\n",
       "      <td>3749.000000</td>\n",
       "      <td>3749.000000</td>\n",
       "      <td>3749.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.135236</td>\n",
       "      <td>0.013604</td>\n",
       "      <td>0.127234</td>\n",
       "      <td>0.054415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.342021</td>\n",
       "      <td>0.115854</td>\n",
       "      <td>0.333279</td>\n",
       "      <td>0.226864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AdaBoost   ExtraTrees  GradientBoost  RandomForest\n",
       "count  3749.000000  3749.000000    3749.000000   3749.000000\n",
       "mean      0.135236     0.013604       0.127234      0.054415\n",
       "std       0.342021     0.115854       0.333279      0.226864\n",
       "min       0.000000     0.000000       0.000000      0.000000\n",
       "25%       0.000000     0.000000       0.000000      0.000000\n",
       "50%       0.000000     0.000000       0.000000      0.000000\n",
       "75%       0.000000     0.000000       0.000000      0.000000\n",
       "max       1.000000     1.000000       1.000000      1.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_predictions_train = pd.DataFrame( {\n",
    "    'RandomForest': rf_oof_train.ravel(),\n",
    "     'ExtraTrees': et_oof_train.ravel(),\n",
    "     'AdaBoost': ada_oof_train.ravel(),\n",
    "      'GradientBoost': gb_oof_train.ravel()\n",
    "    })\n",
    "base_predictions_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorscale": "Viridis",
         "reversescale": true,
         "showscale": true,
         "type": "heatmap",
         "x": [
          "AdaBoost",
          "ExtraTrees",
          "GradientBoost",
          "RandomForest"
         ],
         "y": [
          "AdaBoost",
          "ExtraTrees",
          "GradientBoost",
          "RandomForest"
         ],
         "z": [
          [
           1,
           0.24983022633577048,
           0.6424945376023198,
           0.4140482527243095
          ],
          [
           0.24983022633577048,
           1,
           0.28684348212325267,
           0.46924461740820156
          ],
          [
           0.6424945376023198,
           0.28684348212325267,
           1,
           0.47654354230698254
          ],
          [
           0.4140482527243095,
           0.46924461740820156,
           0.47654354230698254,
           1
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"744ff199-bb82-4b77-8b2e-2d6477ea7df0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"744ff199-bb82-4b77-8b2e-2d6477ea7df0\", [{\"type\": \"heatmap\", \"z\": [[1.0, 0.24983022633577048, 0.6424945376023198, 0.4140482527243095], [0.24983022633577048, 1.0, 0.28684348212325267, 0.46924461740820156], [0.6424945376023198, 0.28684348212325267, 1.0, 0.47654354230698254], [0.4140482527243095, 0.46924461740820156, 0.47654354230698254, 1.0]], \"x\": [\"AdaBoost\", \"ExtraTrees\", \"GradientBoost\", \"RandomForest\"], \"y\": [\"AdaBoost\", \"ExtraTrees\", \"GradientBoost\", \"RandomForest\"], \"colorscale\": \"Viridis\", \"showscale\": true, \"reversescale\": true}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"744ff199-bb82-4b77-8b2e-2d6477ea7df0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"744ff199-bb82-4b77-8b2e-2d6477ea7df0\", [{\"type\": \"heatmap\", \"z\": [[1.0, 0.24983022633577048, 0.6424945376023198, 0.4140482527243095], [0.24983022633577048, 1.0, 0.28684348212325267, 0.46924461740820156], [0.6424945376023198, 0.28684348212325267, 1.0, 0.47654354230698254], [0.4140482527243095, 0.46924461740820156, 0.47654354230698254, 1.0]], \"x\": [\"AdaBoost\", \"ExtraTrees\", \"GradientBoost\", \"RandomForest\"], \"y\": [\"AdaBoost\", \"ExtraTrees\", \"GradientBoost\", \"RandomForest\"], \"colorscale\": \"Viridis\", \"showscale\": true, \"reversescale\": true}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化第一层模型的相关程度\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z= base_predictions_train.astype(float).corr().values ,\n",
    "        x= base_predictions_train.columns.values,\n",
    "        y= base_predictions_train.columns.values,\n",
    "          colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    "    )\n",
    "]\n",
    "\n",
    "py.iplot(data, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成第二层模型的训练测试集\n",
    "x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1)\n",
    "x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhuiling/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8683652697300452"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第二层模型用xgb训练\n",
    "gbm = xgb.XGBClassifier(\n",
    "    #learning_rate = 0.02,\n",
    " n_estimators= 2000,\n",
    " max_depth= 4,\n",
    " min_child_weight= 2,\n",
    " #gamma=1,\n",
    " gamma=0.9,                        \n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread= -1,\n",
    " scale_pos_weight=1).fit(x_train, y_train)\n",
    "predictions = gbm.predict(x_test)\n",
    "\n",
    "0.4*f1_score(y_test,predictions,average='weighted')+0.6*accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得出预测结果 保存到文件\n",
    "# Generate Submission File \n",
    "StackingSubmission = pd.DataFrame({ 'uid': test.uid,\n",
    "                            'label': predictions })\n",
    "StackingSubmission.to_csv(\"./result/baseline_res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dtest = xgb.DMatrix(X_test)\n",
    "#preds =model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存提交结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ID_test['label'] =preds\n",
    "#ID_test['label']=ID_test['label']\n",
    "#ID_test.to_csv('./result/baseline_res.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
