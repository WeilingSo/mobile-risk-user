{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "uid_train = pd.read_csv('../data/uid_train.txt',sep='\\t',header=None,names=('uid','label'))\n",
    "voice_train = pd.read_csv('../data/voice_train.txt',sep='\\t',header=None,names=('uid','opp_num','opp_head','opp_len','start_time','end_time','call_type','in_out'),dtype={'start_time':str,'end_time':str})\n",
    "sms_train = pd.read_csv('../data/sms_train.txt',sep='\\t',header=None,names=('uid','opp_num','opp_head','opp_len','start_time','in_out'),dtype={'start_time':str})\n",
    "wa_train = pd.read_csv('../data/wa_train.txt',sep='\\t',header=None,names=('uid','wa_name','visit_cnt','visit_dura','up_flow','down_flow','wa_type','date'),dtype={'date':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_test = pd.read_csv('../data/voice_test_b.txt',sep='\\t',header=None,names=('uid','opp_num','opp_head','opp_len','start_time','end_time','call_type','in_out'),dtype={'start_time':str,'end_time':str})\n",
    "sms_test = pd.read_csv('../data/sms_test_b.txt',sep='\\t',header=None,names=('uid','opp_num','opp_head','opp_len','start_time','in_out'),dtype={'start_time':str})\n",
    "wa_test = pd.read_csv('../data/wa_test_b.txt',sep='\\t',header=None,names=('uid','wa_name','visit_cnt','visit_dura','up_flow','down_flow','wa_type','date'),dtype={'date':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = np.array(['u'])\n",
    "uid_num = np.arange(7000,10000)\n",
    "uid_num_char = uid_num.astype('U')\n",
    "uid_num_str = np.core.defchararray.add(prefix, uid_num_char)\n",
    "uid_test = pd.DataFrame(uid_num_str, columns=['uid'])\n",
    "uid_test.to_csv('../data/uid_test_a.txt',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = pd.concat([voice_train,voice_test],axis=0)\n",
    "sms = pd.concat([sms_train,sms_test],axis=0)\n",
    "wa = pd.concat([wa_train,wa_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice.start_time = voice.start_time.astype(int)\n",
    "voice.end_time = voice.end_time.astype(int)\n",
    "\n",
    "voice['date'] = voice.start_time//1000000\n",
    "voice['hour'] = voice.start_time%1000000//10000\n",
    "voice['voice_dura']=abs(voice.end_time.astype('int')-voice.start_time.astype('int'))\n",
    "\n",
    "def map_dure_to_cata(x):\n",
    "    if (x < 2):\n",
    "        return 1\n",
    "    elif (x >= 2 & x < 5):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "voice['dura_cls'] = voice['voice_dura'].apply(map_dure_to_cata)\n",
    "\n",
    "sms.start_time = sms.start_time.astype(int)\n",
    "sms['date'] = sms.start_time//1000000\n",
    "sms['hour'] = sms.start_time%1000000//10000\n",
    "\n",
    "wa.date = wa.date.fillna(0).astype(int)\n",
    "wa.up_flow = wa.up_flow.fillna(0).astype(int)\n",
    "wa.down_flow = wa.down_flow.fillna(0).astype(int)\n",
    "wa.visit_dura = wa.visit_dura.fillna(0).astype(int)\n",
    "wa.visit_cnt = wa.visit_cnt.fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dure_to_cata(x):\n",
    "    if (x < 2):\n",
    "        return 1\n",
    "    elif (x >= 2 & x < 10):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "voice['dura_cls'] = voice['voice_dura'].apply(map_dure_to_cata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通话记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  \"\"\"\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    }
   ],
   "source": [
    "# 不同的电话号码数/电话总数\n",
    "voice_opp_num = voice.groupby(['uid'])['opp_num'].agg({'unique_count': lambda x: len(pd.unique(x)),'count':'count'}).add_prefix('voice_opp_num_').reset_index()\n",
    "\n",
    "# 不同的电话号码头三位的数量\n",
    "voice_opp_head=voice.groupby(['uid'])['opp_head'].agg({'unique_count': lambda x: len(pd.unique(x))}).add_prefix('voice_opp_head_').reset_index()\n",
    "\n",
    "# 每种电话长度的通话次数\n",
    "voice_opp_len=voice.groupby(['uid','opp_len'])['uid'].count().unstack().add_prefix('voice_opp_len_').reset_index().fillna(0)\n",
    "\n",
    "# 每种类型通话的次数\n",
    "voice_call_type = voice.groupby(['uid','call_type'])['uid'].count().unstack().add_prefix('voice_call_type_').reset_index().fillna(0)\n",
    "\n",
    "# 每种类型通话的平均时长\n",
    "voice_dura_type = voice.groupby(['uid','call_type'])['voice_dura'].mean().unstack().add_prefix('voice_dura_type_').reset_index().fillna(0)\n",
    "\n",
    "# 接入/打出的电话总数\n",
    "voice_in_out = voice.groupby(['uid','in_out'])['uid'].count().unstack().add_prefix('voice_in_out_').reset_index().fillna(0)\n",
    "\n",
    "# 通话时长的各统计量\n",
    "voice_dura = voice.groupby(['uid'])['voice_dura'].agg(['std','max','median','mean','sum']).add_prefix('voice_dura_').reset_index().fillna(0)\n",
    "\n",
    "## 每个用户收/发电话的号码的不同号码数\n",
    "voice_opp_len_inout = voice.groupby(['uid','in_out'])['opp_num'].agg({'unique_count': lambda x: len(pd.unique(x)),'count':'count'}).unstack().add_prefix('voice_opp_inout_num_').reset_index().fillna(0)\n",
    "\n",
    "# 不同的日期数\n",
    "voice_day_count = voice.groupby(['uid'])['date'].agg({'voice_day_count': lambda x: len(pd.unique(x))}).reset_index().fillna(0)\n",
    "\n",
    "# 每天in/out电话量\n",
    "voice_everyday_in_count = voice[voice.in_out==1].groupby(['uid','date'])['uid'].count().unstack().add_prefix('voice_everyday_in_count').reset_index().fillna(0)\n",
    "voice_everyday_out_count = voice[voice.in_out==0].groupby(['uid','date'])['uid'].count().unstack().add_prefix('voice_everyday_out_count').reset_index().fillna(0)\n",
    "\n",
    "voice_everyday_in_dura = voice[voice.in_out==1].groupby(['uid','date'])['voice_dura'].sum().unstack().add_prefix('voice_everyday_in_dura').reset_index().fillna(0)\n",
    "voice_everyday_out_dura = voice[voice.in_out==0].groupby(['uid','date'])['voice_dura'].sum().unstack().add_prefix('voice_everyday_out_dura').reset_index().fillna(0)\n",
    "\n",
    "# 每个小时段的平均电话量\n",
    "voice_hour_count = voice.groupby(['uid','hour'])['uid'].count().unstack().add_prefix('voice_hour_count').reset_index().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_everyday_call_type = voice.groupby(['uid','date','call_type'])['uid'].count().unstack().add_prefix('voice_everyday_call_type_').reset_index().fillna(0)\n",
    "voice_everyday_call_type_1 = voice_everyday_call_type.groupby(['uid','date'])['voice_everyday_call_type_1'].sum().unstack().add_prefix('day_').reset_index().fillna(0)\n",
    "voice_everyday_call_type_2 = voice_everyday_call_type.groupby(['uid','date'])['voice_everyday_call_type_2'].sum().unstack().add_prefix('day_').reset_index().fillna(0)\n",
    "voice_everyday_call_type_3 = voice_everyday_call_type.groupby(['uid','date'])['voice_everyday_call_type_3'].sum().unstack().add_prefix('day_').reset_index().fillna(0)\n",
    "voice_everyday_call_type_4 = voice_everyday_call_type.groupby(['uid','date'])['voice_everyday_call_type_4'].sum().unstack().add_prefix('day_').reset_index().fillna(0)\n",
    "voice_everyday_call_type_5 = voice_everyday_call_type.groupby(['uid','date'])['voice_everyday_call_type_5'].sum().unstack().add_prefix('day_').reset_index().fillna(0)\n",
    "voice_everyday_call_type = pd.merge(voice_everyday_call_type_1,voice_everyday_call_type_2,how='left',on='uid')\n",
    "voice_everyday_call_type = pd.merge(voice_everyday_call_type,voice_everyday_call_type_3,how='left',on='uid')\n",
    "voice_everyday_call_type = pd.merge(voice_everyday_call_type,voice_everyday_call_type_4,how='left',on='uid')\n",
    "voice_everyday_call_type = pd.merge(voice_everyday_call_type,voice_everyday_call_type_5,how='left',on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每种类型通话的平均时长\n",
    "voice_dura_cls_type = voice.groupby(['uid','call_type'])['dura_cls'].mean().unstack().add_prefix('voice_dura_cls_type_').reset_index().fillna(0)\n",
    "\n",
    "voice_dura_cls0 = voice[voice.in_out==0].groupby(['uid','dura_cls'])['uid'].count().unstack().add_prefix('voice_out_dura_cls_').reset_index().fillna(0)\n",
    "voice_dura_cls1 = voice[voice.in_out==1].groupby(['uid','dura_cls'])['uid'].count().unstack().add_prefix('voice_in_dura_cls_').reset_index().fillna(0)\n",
    "voice_dura_cls = pd.merge(voice_dura_cls0,voice_dura_cls1,how='outer',on='uid').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每种电话类型每种通话时长分类的总通话时长\n",
    "voice_everyday_dura_type = voice.groupby(['uid','dura_cls','call_type'])['uid'].count().unstack().add_prefix('voice_everyday_call_type_').reset_index().fillna(0)\n",
    "voice_everyday_dura_type_1 = voice_everyday_dura_type.groupby(['uid','dura_cls'])['voice_everyday_call_type_1'].sum().unstack().add_prefix('dura_cls_').add_prefix('type1_').reset_index().fillna(0)\n",
    "voice_everyday_dura_type_2 = voice_everyday_dura_type.groupby(['uid','dura_cls'])['voice_everyday_call_type_2'].sum().unstack().add_prefix('dura_cls_').add_prefix('type2_').reset_index().fillna(0)\n",
    "voice_everyday_dura_type_3 = voice_everyday_dura_type.groupby(['uid','dura_cls'])['voice_everyday_call_type_3'].sum().unstack().add_prefix('dura_cls_').add_prefix('type3_').reset_index().fillna(0)\n",
    "voice_everyday_dura_type_4 = voice_everyday_dura_type.groupby(['uid','dura_cls'])['voice_everyday_call_type_4'].sum().unstack().add_prefix('dura_cls_').add_prefix('type4_').reset_index().fillna(0)\n",
    "voice_everyday_dura_type_5 = voice_everyday_dura_type.groupby(['uid','dura_cls'])['voice_everyday_call_type_5'].sum().unstack().add_prefix('dura_cls_').add_prefix('type5_').reset_index().fillna(0)\n",
    "voice_everyday_dura_type = pd.merge(voice_everyday_dura_type_1,voice_everyday_dura_type_2,how='left',on='uid')\n",
    "voice_everyday_dura_type = pd.merge(voice_everyday_dura_type,voice_everyday_dura_type_3,how='left',on='uid')\n",
    "voice_everyday_dura_type = pd.merge(voice_everyday_dura_type,voice_everyday_dura_type_4,how='left',on='uid')\n",
    "voice_everyday_dura_type = pd.merge(voice_everyday_dura_type,voice_everyday_dura_type_5,how='left',on='uid')\n",
    "#voice_everyday_dura_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前k个频繁被拨打的号码，每个用户拨打的时间\n",
    "#voice_opp_num_first_k_ori = voice['opp_num'].value_counts()[0:10]\n",
    "#voice_opp_num_first_k = []\n",
    "#for i in np.arange(0,voice_opp_num_first_k_ori.shape[0]):\n",
    "#    voice_opp_num_first_k.append(voice_opp_num_first_k_ori[i:i+1].to_string().split(' ')[0])\n",
    "\n",
    "#voice_num_else = voice[~voice.opp_num.isin(voice_opp_num_first_k)]\n",
    "#voice_num_first_k_dura = voice_num_else.groupby(['uid'])['voice_dura'].sum().reset_index().fillna(0).rename(columns={'uid': 'uid', 'voice_dura': 'voice_num_else_dura'})\n",
    "    \n",
    "#for num in voice_opp_num_first_k:\n",
    "#    voice_num_k_dura = voice[voice.opp_num == num].groupby(['uid'])['voice_dura'].sum().reset_index().fillna(0)\n",
    "#    voice_num_first_k_dura = pd.merge(voice_num_first_k_dura,voice_num_k_dura,how='outer',on='uid')\n",
    "\n",
    "#voice_num_first_k_dura.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 短信记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# 不同的in/out短信号码数/电话总数\n",
    "sms_out_opp_num = sms[sms.in_out==0].groupby(['uid'])['opp_num'].agg({'unique_count': lambda x: len(pd.unique(x)),'count':'count'}).add_prefix('sms_out_opp_num').reset_index()\n",
    "sms_in_opp_num = sms[sms.in_out==1].groupby(['uid'])['opp_num'].agg({'unique_count': lambda x: len(pd.unique(x)),'count':'count'}).add_prefix('sms_in_opp_num').reset_index()\n",
    "\n",
    "# in/out号码不同头三位的数量\n",
    "sms_opp_head=sms.groupby(['uid','in_out'])['opp_head'].agg({'unique_count': lambda x: len(pd.unique(x))}).unstack().add_prefix('sms_opp_head_').reset_index()\n",
    "\n",
    "# 每种号码长度的短信次数\n",
    "sms_out_opp_len=sms[sms.in_out==0].groupby(['uid','opp_len'])['opp_num'].agg({'unique_count': lambda x: len(pd.unique(x)),'count':'count'}).unstack().add_prefix('sms_out_opp_len').reset_index().fillna(0)\n",
    "sms_in_opp_len=sms[sms.in_out==1].groupby(['uid','opp_len'])['opp_num'].agg({'unique_count': lambda x: len(pd.unique(x)),'count':'count'}).unstack().add_prefix('sms_in_opp_len').reset_index().fillna(0)\n",
    "\n",
    "# 接受/发出短信总数\n",
    "sms_in_out = sms.groupby(['uid','in_out'])['uid'].count().unstack().add_prefix('sms_in_out_').reset_index().fillna(0)\n",
    "\n",
    "# 不同的日期数\n",
    "sms_day_count = sms.groupby(['uid'])['start_time'].agg({'sms_day_count': lambda x: len(pd.unique(x//1000000))}).reset_index().fillna(0)\n",
    "\n",
    "# 每天in/out短信量\n",
    "sms_everyday_out_count = sms[sms.in_out==0].groupby(['uid','date'])['uid'].count().unstack().add_prefix('sms_everyday_out_count').reset_index().fillna(0)\n",
    "sms_everyday_in_count = sms[sms.in_out==1].groupby(['uid','date'])['uid'].count().unstack().add_prefix('sms_everyday_in_count').reset_index().fillna(0)\n",
    "\n",
    "# 每个小时段的平均in/out短信量\n",
    "sms_hour_out_count = sms[sms.in_out==0].groupby(['uid','hour'])['uid'].count().unstack().add_prefix('sms_hour_out_count').reset_index().fillna(0)\n",
    "sms_hour_in_count = sms[sms.in_out==1].groupby(['uid','hour'])['uid'].count().unstack().add_prefix('sms_hour_in_count').reset_index().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py:2530: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# 每5天in/out不同的对端号码\n",
    "sms_everyday_out_opp_head_unique = sms[sms.in_out==0].groupby(['uid','date'])['opp_head'].agg({'unique_count': lambda x: len(pd.unique(x))}).unstack().add_prefix('sms_everyday_out_opp_head_unique').reset_index().fillna(0)\n",
    "sms_everyday_in_opp_head_unique = sms[sms.in_out==1].groupby(['uid','date'])['opp_head'].agg({'unique_count': lambda x: len(pd.unique(x))}).unstack().add_prefix('sms_everyday_in_opp_head_unique').reset_index().fillna(0)\n",
    "\n",
    "out_opp_head_unique = sms_everyday_out_opp_head_unique.drop(['uid'],axis=1)\n",
    "out_opp_head_unique.columns=np.arange(1,46)\n",
    "sms_every5day_out_opp_head_unique = pd.DataFrame()\n",
    "sms_every5day_out_opp_head_unique['uid'] = sms_everyday_out_opp_head_unique.uid\n",
    "sms_every5day_out_opp_head_unique['1'] = out_opp_head_unique.loc[:,1:5].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_unique['2'] = out_opp_head_unique.loc[:,6:10].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_unique['3'] = out_opp_head_unique.loc[:,11:15].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_unique['4'] = out_opp_head_unique.loc[:,16:20].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_unique['5'] = out_opp_head_unique.loc[:,21:25].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_unique['6'] = out_opp_head_unique.loc[:,26:30].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_unique['7'] = out_opp_head_unique.loc[:,31:35].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_unique['8'] = out_opp_head_unique.loc[:,36:40].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_unique['9'] = out_opp_head_unique.loc[:,41:45].apply(lambda x: x.sum(), axis=1)\n",
    "\n",
    "in_opp_head_unique = sms_everyday_in_opp_head_unique.drop(['uid'],axis=1)\n",
    "in_opp_head_unique.columns=np.arange(1,46)\n",
    "sms_every5day_in_opp_head_unique = pd.DataFrame()\n",
    "sms_every5day_in_opp_head_unique['uid'] = sms_everyday_in_opp_head_unique.uid\n",
    "sms_every5day_in_opp_head_unique['1'] = in_opp_head_unique.loc[:,1:5].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_unique['2'] = in_opp_head_unique.loc[:,6:10].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_unique['3'] = in_opp_head_unique.loc[:,11:15].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_unique['4'] = in_opp_head_unique.loc[:,16:20].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_unique['5'] = in_opp_head_unique.loc[:,21:25].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_unique['6'] = in_opp_head_unique.loc[:,26:30].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_unique['7'] = in_opp_head_unique.loc[:,31:35].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_unique['8'] = in_opp_head_unique.loc[:,36:40].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_unique['9'] = in_opp_head_unique.loc[:,41:45].apply(lambda x: x.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每5天in/out的电话数量\n",
    "sms_everyday_out_opp_head_count = sms[sms.in_out==0].groupby(['uid','date'])['opp_head'].count().unstack().add_prefix('sms_everyday_out_opp_head_count').reset_index().fillna(0)\n",
    "sms_everyday_in_opp_head_count = sms[sms.in_out==1].groupby(['uid','date'])['opp_head'].count().unstack().add_prefix('sms_everyday_in_opp_head_count').reset_index().fillna(0)\n",
    "\n",
    "out_opp_head_count = sms_everyday_out_opp_head_count.drop(['uid'],axis=1)\n",
    "out_opp_head_count.columns=np.arange(1,46)\n",
    "sms_every5day_out_opp_head_count = pd.DataFrame({'uid':sms_everyday_out_opp_head_count.uid})\n",
    "sms_every5day_out_opp_head_count['1'] = out_opp_head_count.loc[:,1:5].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_count['2'] = out_opp_head_count.loc[:,6:10].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_count['3'] = out_opp_head_count.loc[:,11:15].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_count['4'] = out_opp_head_count.loc[:,16:20].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_count['5'] = out_opp_head_count.loc[:,21:25].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_count['6'] = out_opp_head_count.loc[:,26:30].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_count['7'] = out_opp_head_count.loc[:,31:35].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_count['8'] = out_opp_head_count.loc[:,36:40].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_out_opp_head_count['9'] = out_opp_head_count.loc[:,41:45].apply(lambda x: x.sum(), axis=1)\n",
    "\n",
    "in_opp_head_count = sms_everyday_in_opp_head_count.drop(['uid'],axis=1)\n",
    "in_opp_head_count.columns=np.arange(1,46)\n",
    "sms_every5day_in_opp_head_count = pd.DataFrame({'uid':sms_everyday_in_opp_head_count.uid})\n",
    "sms_every5day_in_opp_head_count['1'] = in_opp_head_count.loc[:,1:5].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_count['2'] = in_opp_head_count.loc[:,6:10].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_count['3'] = in_opp_head_count.loc[:,11:15].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_count['4'] = in_opp_head_count.loc[:,16:20].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_count['5'] = in_opp_head_count.loc[:,21:25].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_count['6'] = in_opp_head_count.loc[:,26:30].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_count['7'] = in_opp_head_count.loc[:,31:35].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_count['8'] = in_opp_head_count.loc[:,36:40].apply(lambda x: x.sum(), axis=1)\n",
    "sms_every5day_in_opp_head_count['9'] = in_opp_head_count.loc[:,41:45].apply(lambda x: x.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sms_count_by_opp_num \n",
    "avg_count = sms_out_opp_num['sms_out_opp_numcount']/sms_out_opp_num['sms_out_opp_numunique_count']\n",
    "sms_avg_out_count_by_opp_num = pd.DataFrame({'uid':sms_out_opp_num.uid,'sms_avg_out_count_by_opp_num':avg_count})\n",
    "avg_count = sms_in_opp_num['sms_in_opp_numcount']/sms_in_opp_num['sms_in_opp_numunique_count']\n",
    "sms_avg_in_count_by_opp_num = pd.DataFrame({'uid':sms_out_opp_num.uid,'sms_avg_in_count_by_opp_num':avg_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_opp_head_count=sms.groupby(['uid','opp_head'])['uid'].count().unstack().add_prefix('sms_opp_head_').reset_index().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网站/APP记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# 不同wa数量/wa总数\n",
    "w_a_name_cnt = wa.groupby(['uid','wa_type'])['wa_name'].agg({'unique_count': lambda x: len(pd.unique(x)),'count':'count'}).add_prefix('w_name_').unstack().reset_index()\n",
    "\n",
    "# 访问wa次数的各统计量\n",
    "w_a_visit_cnt = wa.groupby(['uid','wa_type'])['visit_cnt'].agg(['std','max','min','median','mean','sum']).add_prefix('w_a_visit_cnt').unstack().reset_index().fillna(0)\n",
    "\n",
    "# 访问w/a时长的各统计量\n",
    "w_a_visit_dura = wa.groupby(['uid','wa_type'])['visit_dura'].agg(['std','max','min','median','mean','sum']).add_prefix('w_a_visit_dura').unstack().reset_index().fillna(0)\n",
    "\n",
    "## 每个用户w/a的上/下的流量\n",
    "w_a_upflow = wa.groupby(['uid','wa_type'])['up_flow'].agg(['std','max','min','median','mean','sum']).add_prefix('w_a_upflow_').unstack().reset_index().fillna(0)\n",
    "w_a_downflow = wa.groupby(['uid','wa_type'])['down_flow'].agg(['std','max','min','median','mean','sum']).add_prefix('w_a_downflow_').unstack().reset_index().fillna(0)\n",
    "\n",
    "# 不同的日期数\n",
    "wa_day_count = wa.groupby(['uid'])['date'].agg({'wa_day_count': lambda x: len(pd.unique(x))}).reset_index().fillna(0)\n",
    "\n",
    "# 每天web上传流量\n",
    "wa_everyday_web_up_flow = wa[wa.wa_type==0].groupby(['uid','date'])['up_flow'].sum().unstack().add_prefix('wa_everyday_web_up_flow').reset_index().fillna(0)\n",
    "\n",
    "# 每天app上传流量\n",
    "wa_everyday_app_up_flow = wa[wa.wa_type==1].groupby(['uid','date'])['up_flow'].sum().unstack().add_prefix('wa_everyday_app_up_flow').reset_index().fillna(0)\n",
    "\n",
    "# 每天web下载流量\n",
    "wa_everyday_web_down_flow = wa[wa.wa_type==0].groupby(['uid','date'])['down_flow'].sum().unstack().add_prefix('wa_everyday_web_down_flow').reset_index().fillna(0)\n",
    "\n",
    "# 每天app下载流量\n",
    "wa_everyday_app_down_flow = wa[wa.wa_type==1].groupby(['uid','date'])['down_flow'].sum().unstack().add_prefix('wa_everyday_app_down_flow').reset_index().fillna(0)\n",
    "\n",
    "# 每天web访问时长\n",
    "wa_everyday_web_visit_dura = wa[wa.wa_type==0].groupby(['uid','date'])['visit_dura'].sum().unstack().add_prefix('wa_everyday_web_visit_dura').reset_index().fillna(0)\n",
    "\n",
    "# 每天app访问时长\n",
    "wa_everyday_app_visit_dura = wa[wa.wa_type==1].groupby(['uid','date'])['visit_dura'].sum().unstack().add_prefix('wa_everyday_app_visit_dura').reset_index().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传流量最多的w名字\n",
    "wa_w = wa[wa.wa_type==0]\n",
    "wa_name = wa_w.groupby(['uid','wa_name'])['up_flow'].sum().unstack().add_prefix('up').reset_index().fillna(0)\n",
    "wa_uid = wa_name.uid;\n",
    "wa_name = wa_name.drop(['uid'],axis=1)\n",
    "col_name = np.arange(0,wa_name.shape[1])\n",
    "col_name = col_name.astype('U')\n",
    "wa_name.columns =col_name\n",
    "wa_name_t = wa_name.T\n",
    "col_name2 = np.arange(0,wa_name_t.shape[1])\n",
    "col_name2 = col_name2.astype('U')\n",
    "wa_name_t.columns =col_name2\n",
    "ss = wa_name_t.idxmax()\n",
    "wa_most_up_webname = pd.DataFrame(wa_uid, columns=['uid'])\n",
    "l = list(ss)\n",
    "wa_most_up_webname['wa_most_up_webname'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传流量最多的a名字\n",
    "wa_w = wa[wa.wa_type==1]\n",
    "wa_name = wa_w.groupby(['uid','wa_name'])['up_flow'].sum().unstack().add_prefix('up').reset_index().fillna(0)\n",
    "wa_uid = wa_name.uid;\n",
    "wa_name = wa_name.drop(['uid'],axis=1)\n",
    "col_name = np.arange(0,wa_name.shape[1])\n",
    "col_name = col_name.astype('U')\n",
    "wa_name.columns =col_name\n",
    "wa_name_t = wa_name.T\n",
    "col_name2 = np.arange(0,wa_name_t.shape[1])\n",
    "col_name2 = col_name2.astype('U')\n",
    "wa_name_t.columns =col_name2\n",
    "ss = wa_name_t.idxmax()\n",
    "wa_most_up_appname = pd.DataFrame(wa_uid, columns=['uid'])\n",
    "l = list(ss)\n",
    "wa_most_up_appname['wa_most_up_appname'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载流量最多的w名字\n",
    "wa_w = wa[wa.wa_type==0]\n",
    "wa_name = wa_w.groupby(['uid','wa_name'])['down_flow'].sum().unstack().add_prefix('down').reset_index().fillna(0)\n",
    "wa_uid = wa_name.uid;\n",
    "wa_name = wa_name.drop(['uid'],axis=1)\n",
    "col_name = np.arange(0,wa_name.shape[1])\n",
    "col_name = col_name.astype('U')\n",
    "wa_name.columns =col_name\n",
    "wa_name_t = wa_name.T\n",
    "col_name2 = np.arange(0,wa_name_t.shape[1])\n",
    "col_name2 = col_name2.astype('U')\n",
    "wa_name_t.columns =col_name2\n",
    "ss = wa_name_t.idxmax()\n",
    "wa_most_down_webname = pd.DataFrame(wa_uid, columns=['uid'])\n",
    "l = list(ss)\n",
    "wa_most_down_webname['wa_most_down_webname'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载流量最多的a名字\n",
    "wa_w = wa[wa.wa_type==1]\n",
    "wa_name = wa_w.groupby(['uid','wa_name'])['down_flow'].sum().unstack().add_prefix('down').reset_index().fillna(0)\n",
    "wa_uid = wa_name.uid;\n",
    "wa_name = wa_name.drop(['uid'],axis=1)\n",
    "col_name = np.arange(0,wa_name.shape[1])\n",
    "col_name = col_name.astype('U')\n",
    "wa_name.columns =col_name\n",
    "wa_name_t = wa_name.T\n",
    "col_name2 = np.arange(0,wa_name_t.shape[1])\n",
    "col_name2 = col_name2.astype('U')\n",
    "wa_name_t.columns =col_name2\n",
    "ss = wa_name_t.idxmax()\n",
    "wa_most_down_appname = pd.DataFrame(wa_uid, columns=['uid'])\n",
    "l = list(ss)\n",
    "wa_most_down_appname['wa_most_down_appname'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 访问次数最多的w名字\n",
    "wa_w = wa[wa.wa_type==0]\n",
    "wa_name = wa_w.groupby(['uid','wa_name'])['visit_cnt'].sum().unstack().add_prefix('cnt').reset_index().fillna(0)\n",
    "wa_uid = wa_name.uid;\n",
    "wa_name = wa_name.drop(['uid'],axis=1)\n",
    "col_name = np.arange(0,wa_name.shape[1])\n",
    "col_name = col_name.astype('U')\n",
    "wa_name.columns =col_name\n",
    "wa_name_t = wa_name.T\n",
    "col_name2 = np.arange(0,wa_name_t.shape[1])\n",
    "col_name2 = col_name2.astype('U')\n",
    "wa_name_t.columns =col_name2\n",
    "ss = wa_name_t.idxmax()\n",
    "wa_most_cnt_webname = pd.DataFrame(wa_uid, columns=['uid'])\n",
    "l = list(ss)\n",
    "wa_most_cnt_webname['wa_most_cnt_webname'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 访问次数最多的a名字\n",
    "wa_w = wa[wa.wa_type==1]\n",
    "wa_name = wa_w.groupby(['uid','wa_name'])['visit_cnt'].sum().unstack().add_prefix('cnt').reset_index().fillna(0)\n",
    "wa_uid = wa_name.uid;\n",
    "wa_name = wa_name.drop(['uid'],axis=1)\n",
    "col_name = np.arange(0,wa_name.shape[1])\n",
    "col_name = col_name.astype('U')\n",
    "wa_name.columns =col_name\n",
    "wa_name_t = wa_name.T\n",
    "col_name2 = np.arange(0,wa_name_t.shape[1])\n",
    "col_name2 = col_name2.astype('U')\n",
    "wa_name_t.columns =col_name2\n",
    "ss = wa_name_t.idxmax()\n",
    "wa_most_cnt_appname = pd.DataFrame(wa_uid, columns=['uid'])\n",
    "l = list(ss)\n",
    "wa_most_cnt_appname['wa_most_cnt_appname'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 访问时长最多的w名字\n",
    "wa_w = wa[wa.wa_type==0]\n",
    "wa_name = wa_w.groupby(['uid','wa_name'])['visit_dura'].sum().unstack().add_prefix('cnt').reset_index().fillna(0)\n",
    "wa_uid = wa_name.uid;\n",
    "wa_name = wa_name.drop(['uid'],axis=1)\n",
    "col_name = np.arange(0,wa_name.shape[1])\n",
    "col_name = col_name.astype('U')\n",
    "wa_name.columns =col_name\n",
    "wa_name_t = wa_name.T\n",
    "col_name2 = np.arange(0,wa_name_t.shape[1])\n",
    "col_name2 = col_name2.astype('U')\n",
    "wa_name_t.columns =col_name2\n",
    "ss = wa_name_t.idxmax()\n",
    "wa_most_dura_webname = pd.DataFrame(wa_uid, columns=['uid'])\n",
    "l = list(ss)\n",
    "wa_most_dura_webname['wa_most_dura_webname'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 访问时长最多的a名字\n",
    "wa_w = wa[wa.wa_type==1]\n",
    "wa_name = wa_w.groupby(['uid','wa_name'])['visit_dura'].sum().unstack().add_prefix('cnt').reset_index().fillna(0)\n",
    "wa_uid = wa_name.uid;\n",
    "wa_name = wa_name.drop(['uid'],axis=1)\n",
    "col_name = np.arange(0,wa_name.shape[1])\n",
    "col_name = col_name.astype('U')\n",
    "wa_name.columns =col_name\n",
    "wa_name_t = wa_name.T\n",
    "col_name2 = np.arange(0,wa_name_t.shape[1])\n",
    "col_name2 = col_name2.astype('U')\n",
    "wa_name_t.columns =col_name2\n",
    "ss = wa_name_t.idxmax()\n",
    "wa_most_dura_appname = pd.DataFrame(wa_uid, columns=['uid'])\n",
    "l = list(ss)\n",
    "wa_most_dura_appname['wa_most_dura_appname'] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前45种wa和其他剩余wa的dura\n",
    "wa_w = wa[wa.wa_type==1]\n",
    "wa_name_first_k_ori = wa_w['wa_name'].value_counts()[0:30]\n",
    "wa_name_first_k = []\n",
    "for i in np.arange(0,wa_name_first_k_ori.shape[0]):\n",
    "    wa_name_first_k.append(wa_name_first_k_ori[i:i+1].to_string().split(' ')[0])\n",
    "\n",
    "wa_name_else = wa_w[~wa_w.wa_name.isin(wa_name_first_k)]\n",
    "wa_name_first_k_w_dura = wa_name_else.groupby(['uid'])['visit_dura'].sum().reset_index().fillna(0).rename(columns={'uid': 'uid', 'visit_dura': 'wa_name_else_dura'})\n",
    "    \n",
    "for num in wa_name_first_k:\n",
    "    wa_name_k_dura = wa_w[wa_w.wa_name == num].groupby(['uid'])['visit_dura'].sum().reset_index().fillna(0)\n",
    "    wa_name_first_k_w_dura = pd.merge(wa_name_first_k_w_dura,wa_name_k_dura,how='outer',on='uid')\n",
    "\n",
    "wa_name_first_k_w_dura.fillna(0,inplace=True)\n",
    "#wa_name_first_k_w_dura.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前45种wa和其他剩余wa的dura\n",
    "wa_w = wa[wa.wa_type==0]\n",
    "wa_name_first_k_ori = wa_w['wa_name'].value_counts()[0:30]\n",
    "wa_name_first_k = []\n",
    "for i in np.arange(0,wa_name_first_k_ori.shape[0]):\n",
    "    wa_name_first_k.append(wa_name_first_k_ori[i:i+1].to_string().split(' ')[0])\n",
    "\n",
    "wa_name_else = wa_w[~wa_w.wa_name.isin(wa_name_first_k)]\n",
    "wa_name_first_k_a_dura = wa_name_else.groupby(['uid'])['visit_dura'].sum().reset_index().fillna(0).rename(columns={'uid': 'uid', 'visit_dura': 'wa_name_else_dura'})\n",
    "    \n",
    "for num in wa_name_first_k:\n",
    "    wa_name_k_dura = wa_w[wa_w.wa_name == num].groupby(['uid'])['visit_dura'].sum().reset_index().fillna(0)\n",
    "    wa_name_first_k_a_dura = pd.merge(wa_name_first_k_a_dura,wa_name_k_dura,how='outer',on='uid')\n",
    "\n",
    "wa_name_first_k_a_dura.fillna(0,inplace=True)\n",
    "#wa_name_first_k_a_dura.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将各个特征拼在一起\n",
    "feature = [voice_opp_num,voice_opp_head,voice_opp_len,voice_call_type,voice_in_out,voice_dura,\n",
    "           voice_opp_len_inout,voice_dura_type,voice_day_count,voice_everyday_out_count,voice_everyday_in_count,\n",
    "           voice_hour_count,voice_everyday_in_dura,voice_everyday_out_dura,voice_everyday_call_type,\n",
    "           voice_dura_cls_type,voice_dura_cls,voice_everyday_dura_type,\n",
    "           sms_out_opp_num,sms_in_opp_num,sms_opp_head,sms_out_opp_len,sms_in_opp_len,sms_in_out,\n",
    "           sms_day_count,sms_everyday_out_count,sms_everyday_in_count,sms_hour_out_count,sms_hour_in_count,\n",
    "            #6.7 add\n",
    "           sms_every5day_out_opp_head_unique,sms_every5day_in_opp_head_unique,\n",
    "           sms_every5day_out_opp_head_count,sms_every5day_in_opp_head_count,\n",
    "           sms_avg_out_count_by_opp_num,sms_avg_in_count_by_opp_num,\n",
    "           sms_opp_head_count,\n",
    "           w_a_name_cnt,w_a_visit_cnt,w_a_visit_dura,w_a_upflow,w_a_downflow,\n",
    "           wa_day_count,wa_everyday_web_up_flow,\n",
    "           wa_everyday_app_up_flow,wa_everyday_web_down_flow,wa_everyday_app_down_flow,\n",
    "           wa_everyday_web_visit_dura,wa_everyday_app_visit_dura,wa_most_up_webname,wa_most_down_webname,\n",
    "           wa_most_cnt_webname,wa_most_dura_webname,wa_most_up_appname,wa_most_down_appname,wa_most_cnt_appname,\n",
    "           wa_most_dura_appname,\n",
    "           # 6.7add\n",
    "           wa_name_first_k_w_dura,wa_name_first_k_a_dura,\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py:2530: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# 将train特征和test特征分离\n",
    "train_feature = uid_train\n",
    "for feat in feature:\n",
    "    train_feature=pd.merge(train_feature,feat,how='left',on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py:2530: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "test_feature = uid_test\n",
    "for feat in feature:\n",
    "    test_feature=pd.merge(test_feature,feat,how='left',on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填补Nan\n",
    "train_feature.fillna(0,inplace=True)\n",
    "test_feature.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存特征值\n",
    "train_feature.to_csv('../data/train_featureV1.csv',index=None)\n",
    "test_feature.to_csv('../data/test_featureV1.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>label</th>\n",
       "      <th>voice_opp_num_unique_count</th>\n",
       "      <th>voice_opp_num_count</th>\n",
       "      <th>voice_opp_head_unique_count</th>\n",
       "      <th>voice_opp_len_3</th>\n",
       "      <th>voice_opp_len_5</th>\n",
       "      <th>voice_opp_len_6</th>\n",
       "      <th>voice_opp_len_7</th>\n",
       "      <th>voice_opp_len_8</th>\n",
       "      <th>...</th>\n",
       "      <th>visit_dura_x_y</th>\n",
       "      <th>visit_dura_y_y</th>\n",
       "      <th>visit_dura_x_y</th>\n",
       "      <th>visit_dura_y_y</th>\n",
       "      <th>visit_dura_x_y</th>\n",
       "      <th>visit_dura_y_y</th>\n",
       "      <th>visit_dura_x_y</th>\n",
       "      <th>visit_dura_y_y</th>\n",
       "      <th>visit_dura_x_y</th>\n",
       "      <th>visit_dura_y_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0001</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47692.0</td>\n",
       "      <td>13190.0</td>\n",
       "      <td>17314.0</td>\n",
       "      <td>14337.0</td>\n",
       "      <td>34536.0</td>\n",
       "      <td>17084.0</td>\n",
       "      <td>4806.0</td>\n",
       "      <td>9402.0</td>\n",
       "      <td>3378084.0</td>\n",
       "      <td>8907.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0002</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0003</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25104.0</td>\n",
       "      <td>17164.0</td>\n",
       "      <td>18219.0</td>\n",
       "      <td>38717.0</td>\n",
       "      <td>1039108.0</td>\n",
       "      <td>440294.0</td>\n",
       "      <td>175357.0</td>\n",
       "      <td>2184.0</td>\n",
       "      <td>452927.0</td>\n",
       "      <td>2281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0004</td>\n",
       "      <td>0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>145443.0</td>\n",
       "      <td>22237.0</td>\n",
       "      <td>10694.0</td>\n",
       "      <td>513625.0</td>\n",
       "      <td>110245.0</td>\n",
       "      <td>440247.0</td>\n",
       "      <td>607104.0</td>\n",
       "      <td>15360.0</td>\n",
       "      <td>1971461.0</td>\n",
       "      <td>5034.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u0005</td>\n",
       "      <td>0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2343340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4844.0</td>\n",
       "      <td>82539.0</td>\n",
       "      <td>9950.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>174052.0</td>\n",
       "      <td>160872.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid  label  voice_opp_num_unique_count  voice_opp_num_count  \\\n",
       "0  u0001      0                        22.0                 79.0   \n",
       "1  u0002      0                         2.0                  2.0   \n",
       "2  u0003      0                        15.0                 21.0   \n",
       "3  u0004      0                        77.0                254.0   \n",
       "4  u0005      0                        55.0                401.0   \n",
       "\n",
       "   voice_opp_head_unique_count  voice_opp_len_3  voice_opp_len_5  \\\n",
       "0                         17.0              0.0              3.0   \n",
       "1                          1.0              0.0              0.0   \n",
       "2                         10.0              0.0              1.0   \n",
       "3                         31.0              0.0              1.0   \n",
       "4                         28.0              0.0              4.0   \n",
       "\n",
       "   voice_opp_len_6  voice_opp_len_7  voice_opp_len_8       ...        \\\n",
       "0              0.0              0.0              0.0       ...         \n",
       "1              0.0              0.0              0.0       ...         \n",
       "2              0.0              0.0              2.0       ...         \n",
       "3              0.0              0.0             12.0       ...         \n",
       "4              0.0              0.0              0.0       ...         \n",
       "\n",
       "   visit_dura_x_y  visit_dura_y_y  visit_dura_x_y  visit_dura_y_y  \\\n",
       "0         47692.0         13190.0         17314.0         14337.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2         25104.0         17164.0         18219.0         38717.0   \n",
       "3        145443.0         22237.0         10694.0        513625.0   \n",
       "4       2343340.0             0.0             0.0          4844.0   \n",
       "\n",
       "   visit_dura_x_y  visit_dura_y_y  visit_dura_x_y  visit_dura_y_y  \\\n",
       "0         34536.0         17084.0          4806.0          9402.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2       1039108.0        440294.0        175357.0          2184.0   \n",
       "3        110245.0        440247.0        607104.0         15360.0   \n",
       "4         82539.0          9950.0             0.0          1958.0   \n",
       "\n",
       "   visit_dura_x_y  visit_dura_y_y  \n",
       "0       3378084.0          8907.0  \n",
       "1             0.0             0.0  \n",
       "2        452927.0          2281.0  \n",
       "3       1971461.0          5034.0  \n",
       "4        174052.0        160872.0  \n",
       "\n",
       "[5 rows x 1221 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
